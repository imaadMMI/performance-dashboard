--- PAGE 1 ---
 
 
For all of these example designs I have given below, we will need to adapt the 
colour scheme/font to match national bonds 
 
When the user clicks ‘speak to Nada’ - they will enter a conversation with our AI 
voice agent. This AI voice agent will help the National Bonds employee improve 
their customer interaction skills: it will generate ‘roleplaying’ scenarios as a 
customer, using the main strengths/weaknesses that we analysed for this 
individual employee during their real customer calls. It will then give improvement 
and appreciation feedback based on the employee’s performance. 
 
IMPORTANT: There are no AI agent implemented yet. Now use microphone to 
animate the circle in the center. Prepare also some hardcoded dialog .json files 
to show them on the screen. 
 
I have design some screens to help guide this: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- PAGE 2 ---
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(for the left hand side side-bar, when you click the icon, the full side bar 
opens, and when you click the icon again, the side-bar collapses) 
 


--- PAGE 3 ---
 
 
 
(in this page, when you click the message icon in the bottom right, you can open these 
options for ‘pick a pathway’ - here we will include the possible roleplaying/coaching 
scenarios for the employee) 


--- PAGE 4 ---
 
 
 
Once the conversation starts, these icons appear to mute the 
conversation/end the call 
 
We will need to design the ‘voice orb’ (i.e. the circle in the middle) so that it 
modulates in size depending on the speaking of the user. There should be 
pre-built libraries which handle this modulation - I think we can speak to 
piotr about the best way to handle this - he says there are many standard 
solutions online. 
 
 
 
 
 
 
 
 
 
 


--- PAGE 5 ---
 
 
 
It would be great if we could implement a change in the UI for when the 
human user is speaking, vs when they are listening. 
 
When the human user is speaking - we could use a page like the one above 
- where the voice orb changes colour slightly and there is a waveform for 
the audio they are speaking (we have this in the current elara application - 
we can ask piotr how to implement this) 
 
But when the voice avatar ‘Elara’ is speaking, and the human user is 
listening, we can implement this UI design:  
 
 


--- PAGE 6 ---
 
 
 
 
Where we show the transcript live under the voice orb 
 
*** 
 
Once the conversation has started, the ‘message’ icon in the bottom right 
could transition into displaying the following transcript and emotion 
measurements when you click on it:  
 
 
 
 
And when the conversation finishes, clicking on the 
message icon could display the summary of the 
conversation:  
 
 
 
 


--- PAGE 7 ---
 
 
 
Here is an example conversation summary - we can iterate on this more 
and design exactly how this fits into the current design 
 
 
 
 
 
 
 
 


